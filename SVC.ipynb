{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebede436",
   "metadata": {},
   "source": [
    "# Classification using `SupportVectorClassifier` on `Titanic Dataset`\n",
    "## üõ† Project Overview\n",
    "\n",
    "In this project, I am building a Support Vector Machine (SVM) model to predict survival on the Titanic dataset. The workflow includes:\n",
    "\n",
    "1. **Data Loading & Exploration**\n",
    "2. **Handling Missing Values**\n",
    "3. **Feature Engineering** (dropping irrelevant columns & encoding categorical features)\n",
    "4. **Feature Scaling**\n",
    "5. **Visualization** (PCA projection and feature plots)\n",
    "6. **Model Training** with `SVC`\n",
    "7. **Model Evaluation** (classification report & accuracy score)\n",
    "\n",
    "You can **download** the **dataset** directly from [Kaggle](https://www.kaggle.com/competitions/titanic/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4e7df",
   "metadata": {},
   "source": [
    "## üì¶ Import Required Libraries\n",
    "\n",
    "We start by importing all the required libraries for data manipulation, visualization, preprocessing, and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Disabling warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3e150",
   "metadata": {},
   "source": [
    "# üì• Load the Dataset\n",
    "\n",
    "Read the Titanic training dataset (`train.csv`) and display the first few rows to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37874d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "df_train = pd.read_csv(\"Titanic Dataset/train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b92c7bd",
   "metadata": {},
   "source": [
    "## üìä Dataset Information\n",
    "\n",
    "Check dataset information (column names, non-null counts, and datatypes) using `.info()`. This helps in identifying categorical and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823cfe85",
   "metadata": {},
   "source": [
    "## üìà Summary Statistics\n",
    "\n",
    "Use `.describe()` to view summary statistics of numerical columns (mean, std, min, max, quartiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d257300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612f0b2",
   "metadata": {},
   "source": [
    "## ‚ùì Missing Values Check\n",
    "\n",
    "Check how many missing values are present in each column. Handling missing values properly is critical for improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d042ee",
   "metadata": {},
   "source": [
    "## üîß Handle Missing Values ‚Äî `Age`\n",
    "\n",
    "The `Age` column contains missing values. Instead of filling them with a single statistic, we fill each missing `Age` with the `median` age grouped by `Pclass` and `Sex`, as these features are strongly correlated with age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646dbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling `Age` column\n",
    "df_train[\"Age\"] = df_train[\"Age\"].fillna(\n",
    "    df_train.groupby([\"Pclass\", \"Sex\"])[\"Age\"].transform(\"median\")\n",
    ")\n",
    "\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b38c0d",
   "metadata": {},
   "source": [
    "## üîß Handle Missing Values ‚Äî Embarked\n",
    "\n",
    "The `Embarked` column has a small number of missing values. We fill them using the mode (most frequent port of embarkation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling ``Embarked`` column with mode\n",
    "df_train[\"Embarked\"] = df_train[\"Embarked\"].fillna(df_train[\"Embarked\"].mode()[0])\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad7be0",
   "metadata": {},
   "source": [
    "## üóë Remove Irrelevant Columns\n",
    "\n",
    "Drop irrelevant or high-missing-value columns that won‚Äôt add value to the prediction:\n",
    "\n",
    "- `PassengerId` (just an identifier)\n",
    "- `Name` (too diverse)\n",
    "- `Ticket` (not meaningful for survival prediction)\n",
    "- `Cabin` (too many missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301a449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is time to remove all unnecessary columns\n",
    "df_train.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], inplace=True)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba3fd1",
   "metadata": {},
   "source": [
    "## üîÑ One-Hot Encoding\n",
    "\n",
    "Convert categorical variables (`Sex`, `Embarked`) into numerical dummy variables using `One-Hot Encoding`. This prepares the dataset for machine learning algorithms that require numeric input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd588709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for categorical columns\n",
    "df_train = pd.get_dummies(df_train,columns=['Sex', 'Embarked'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0ac4a",
   "metadata": {},
   "source": [
    "## üéØ Define Features and Target\n",
    "\n",
    "- Target (`y`) = `Survived`\n",
    "- Features (`X`) = All remaining numeric columns after preprocessing\n",
    "\n",
    "We also apply `StandardScaler` to standardize the feature values, which helps SVM perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780fc32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features and target variable\n",
    "X = df_train.drop(\"Survived\", axis=1)\n",
    "y = df_train[\"Survived\"]\n",
    "\n",
    "# Scale Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef9bad",
   "metadata": {},
   "source": [
    "## üîç PCA Visualization\n",
    "\n",
    "Apply Principal Component Analysis (PCA) to reduce features to 2D for visualization.\n",
    "This scatter plot gives an idea of how survival outcomes are distributed in reduced feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac0e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "A = df_train.drop('Survived', axis=1).select_dtypes(include=['int64','float64'])\n",
    "B = df_train['Survived']\n",
    "\n",
    "# PCA reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(A)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=B, cmap='coolwarm', s=50)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.title('Titanic Data (PCA Projection)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195a928",
   "metadata": {},
   "source": [
    "## üìä Age vs Fare Visualization\n",
    "\n",
    "Plot `Age` vs `Fare`, coloring points by `survival` outcome. This helps visualize how these two features relate to survival probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_train['Age'], df_train['Fare'], \n",
    "            c=df_train['Survived'], cmap='bwr', s=50)\n",
    "\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Fare\")\n",
    "plt.title(\"Titanic Dataset: Age vs Fare (colored by Survival)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b4a97",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Train-Test Split\n",
    "\n",
    "Split the dataset into `training` (65%) and `testing` (35%) sets.\n",
    "This allows us to train the model on one part of the data and evaluate performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
    "\n",
    "# Training the SVC model with `linear` kernal\n",
    "model = SVC(kernel = 'rbf', class_weight = 'balanced', C = 1.0, gamma = 0.001, random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Printing Success Message\n",
    "print(\"Model Trained Successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0056e",
   "metadata": {},
   "source": [
    "## ü§ñ Train the SVM Model\n",
    "\n",
    "Train a Support Vector Classifier (SVC) with:\n",
    "\n",
    "- `Kernel` = `RBF`\n",
    "- `class_weight` = `'balanced'` (to handle class imbalance)\n",
    "- `C` = `1.0` (regularization parameter)\n",
    "- `gamma` = `0.001` (influence of a single training example)\n",
    "\n",
    "## üìà Model Evaluation\n",
    "\n",
    "Make predictions on the test set and evaluate using:\n",
    "\n",
    "- `Classification Report` (precision, recall, F1-score)\n",
    "- `Accuracy Score`\n",
    "\n",
    "This shows how well the model predicts survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the values and printing the classification report and accuracy score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a1c99",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusion  \n",
    "\n",
    "The Support Vector Classifier (SVC) with an **RBF kernel** achieved an **accuracy of ~73%** on the Titanic dataset.  \n",
    "\n",
    "### üìä Performance Breakdown  \n",
    "- **Class 0 (Did Not Survive):**\n",
    "  - Precision: **0.77**\n",
    "  - Recall: **0.79**\n",
    "  - F1-score: **0.78**\n",
    "- **Class 1 (Survived):**\n",
    "  - Precision: **0.67**\n",
    "  - Recall: **0.65**\n",
    "  - F1-score: **0.66**\n",
    "\n",
    "### üîé Observations  \n",
    "- The model performs **slightly better at predicting non-survivors (Class 0)** compared to survivors (Class 1).  \n",
    "- This imbalance is expected since the dataset itself is slightly skewed toward non-survivors.  \n",
    "- **Feature scaling and missing value imputation** helped improve the model‚Äôs stability.  \n",
    "\n",
    "### üí° Possible Improvements  \n",
    "- Apply **hyperparameter tuning** (GridSearchCV / RandomizedSearchCV) for optimal `C`, `gamma`, and kernel values.  \n",
    "- Experiment with **other models** such as Random Forest, Gradient Boosting, or Logistic Regression for comparison.  \n",
    "- Perform **feature engineering** (e.g., FamilySize, Title from Name, Fare per Person) to capture hidden patterns.  \n",
    "- Use **cross-validation** for more robust and generalized performance evaluation.  \n",
    "\n",
    "### üìå Final Note  \n",
    "With **73% accuracy**, the model demonstrates **reasonable predictive power**, but there is **room for improvement** through tuning and advanced feature engineering.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
